{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Linear Regression With single feature\n'''the model is f(x)= (w*x)+b\n   the cost function j= (1/(2*m)) summation of i=1 to m (f(x)-y)**2\n   \n   the gradient descent algorithm is:\n   \n   untill converges{\n   w=w-(alpha * d_dw)  d_dw====> (1/m) * summation of i=1 to m (f(x)-y)*x\n   b=b-(alpha * d_db)  d_db====> (1/m) * summation of i=1 to m (f(x)-y)\n   }\n    '''\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\ndef cost_function(x,y,w,b):\n    m=len(x)\n    j=0\n    for i in range(m):\n        j+=(w*x[i]+b-y[i])**2\n    j=j/(2*m)\n    \n    return j\n\ndef compute_gradient(x,y,w,b):\n    d_dw=0\n    d_db=0\n    m=len(x)\n    for i in range(m):\n        f_wb=w*x[i]+b\n        d_db+=f_wb-y[i]\n        d_dw+=(f_wb-y[i])*x[i]\n    d_db=d_db/m\n    d_dw=d_dw/m\n    return d_dw, d_db\n\ndef gradient_descent(x,y,w,b,iters):\n    alpha=0.000001\n    j=[]\n    wl=[]\n    prev_cost=cost_function(x,y,w,b)\n    for i in range(iters):\n        w_gradient,b_gradient=compute_gradient(x,y,w,b)\n        w=w-(alpha*w_gradient)\n        b=b-(alpha*b_gradient)\n        curr_cost=cost_function(x,y,w,b)\n        j.append(curr_cost)\n        wl.append(w)\n        if(abs(prev_cost-curr_cost)<1e-7):\n            break\n        prev_cost=curr_cost\n    return w,b, j,wl\n\n\nx = [2000, 2100, 2500, 2250, 3000, 1900, 1500, 1850, 2700, 2100, 2150, 2100, 2100, 2510, 2250, 3100, 1800, 1550, 1800, 2700, 2110, 2100, 3500, 1200, 2800, 3100, 2750, 1800, 2200, 3100, 2100, 2100, 2500, 2250, 3000, 1900, 1500, 1850, 2700, 2100, 2150, 2100, 2100, 2510, 2250, 3100, 1800, 1550, 1800, 2700, 2110, 2100, 3500, 1200, 2800, 3100, 2750, 1800, 2200, 3100]\ny= [31500, 35000, 41050, 36100, 52100, 32500, 20000, 24500, 48000, 31000, 34500, 32000, 34500, 40050, 34100, 51500, 30500, 21000, 25000, 47000, 31500, 33500, 70000, 20000, 50000, 53000, 48000, 25000, 31460, 51400, 33500, 35010, 41100, 35100, 52200, 32300, 20200, 24000, 47500, 31500, 34400, 32020, 34700, 40000, 35000, 51000, 30000, 21500, 25500, 47500, 31000, 33000, 70500, 20100, 51000, 54000, 48500, 25100, 31560, 51600]\nplt.scatter(x,y)\nplt.show()\nx=x/np.max(x)\ny=y/np.max(y)\n\nw=0.25\nb=0.25\n\nw,b,j,wl=gradient_descent(x,y,w,b,100000)\n\nx_input=float(input(\"Enter house size: \"))\nprint(\"The w and b is: \",w,b)\nprediction= (w*x_input)+b\nprint(\"Number of Iterations: \",len(j))\nprint(\"The predicted rent is: \",prediction)\n\nplt.plot(wl,j)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T05:13:57.665165Z","iopub.execute_input":"2023-07-14T05:13:57.666137Z","iopub.status.idle":"2023-07-14T05:54:49.719347Z","shell.execute_reply.started":"2023-07-14T05:13:57.666097Z","shell.execute_reply":"2023-07-14T05:54:49.718072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear Regression with multiple variable\n\n'''the model is f(x)= (w_vector.x_vector)+b\n   the cost function j= (1/(2*m)) summation of i=1 to m (f(x)-y)**2\n   \n   the gradient descent algorithm is:\n   \n   untill converges{\n   w=w-(alpha * d_dw)  d_dw====> (1/m) * summation of i=1 to m (f(x)-y)*x\n   b=b-(alpha * d_db)  d_db====> (1/m) * summation of i=1 to m (f(x)-y)\n   }\n    '''\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n'''The main purpose of Feature scaling is to normlize the data set to have similar range between 0 and 1'''\ndef feature_scale(x):\n    x_max=np.max(x,axis=0)\n    return x/x_max\n\ndef cost_function(x,y,w,b):\n    j=0\n    m=x.shape[0]\n    for i in range(m):\n        j+=(np.dot(w,x[i])+b-y[i])**2\n    j=j/(2*m)\n    return j\n\ndef compute_gradient(x,y,w,b):\n    m,n=x.shape    # m represents number of inputs, n represents number of features\n    d_dw=np.zeros((n,))\n    d_db=0\n    \n    '''let x = ([[1,2,3], [4,5,6]]) and w=[0,-1,1,-2]\n    in the outer for loop the error is calculated between x[i] and w which is dot product between [1,2,3] and w\n    and similar for next iteration.\n    in the inner for loop the gradient for each w is calculated with the corresponding x for example in w [0,-1,1,-2]\n    when the loop runs for the first time, it will update the 1st element in d_dw with respect to the each 1st element in i inputs\n    '''\n    for i in range(m):  # since the data set is too large\n        err= np.dot(w,x[i]+b-y[i])\n        for j in range(n):\n            d_dw[j]+=err*x[i,j]\n        d_db+=err\n    d_dw/=m\n    d_db/=m\n    \n    '''for i in range(m):\n            err=np.dot(w,x[i])+b-y[i]\n            d_dw+=err*x[i]\n            d_db+=err\n        d_dw/=m\n        d_db/=m\n        \n        this also works fine'''\n    \n    return d_dw,d_db\n\ndef gradient_descent(x,y,w,b,alpha,iters):\n    j=[]\n    it=[]\n    prev_cost=cost_function(x,y,w,b)\n    for i in range(iters):\n        w_gradient, b_gradient=compute_gradient(x,y,w,b)\n        w=w-(alpha*w_gradient)\n        b=b-(alpha*b_gradient)\n        curr_cost=cost_function(x,y,w,b)\n        j.append(curr_cost)\n        it.append(i)\n        if abs(prev_cost-curr_cost)<1e-7:\n            break\n        prev_cost=curr_cost\n    \n    return w,b,j,it\n\n\ndata= pd.read_csv('/kaggle/input/house-rent-prediction-dataset/House_Rent_Dataset.csv')\n\nx_cols = ['BHK', 'Size', 'Area Type', 'Furnishing Status', 'Bathroom']\nx = data[x_cols].copy()  # Make a copy of the selected columns\n\nx['Area Type'] = x['Area Type'].replace({'Super Area': 2, 'Built Area':1, 'Carpet Area': 0})\nx['Furnishing Status'] = x['Furnishing Status'].replace({'Furnished': 2, 'Semi-Furnished': 1, 'Unfurnished': 0})\n\ny = data['Rent']\n\nx=np.array(x)\ny=np.array(y)\n\n\nx=feature_scale(x)\ny=y/np.max(y)\n\nb = 0.25\nw = np.array([ 0.25,0.25,0.25,0.25, 0.25])\nalpha=0.0001\niters=100000\nw,b,j,it=gradient_descent(x,y,w,b,alpha,iters)\nprint(\"the w is:\",*w)\nprint(\"the b is:\", b)\n\nplt.plot(it,j)\nplt.show()\n\nprint(\"Number Of Iterations to Converge: \", it[-1]+1)\n\nx_input=np.array(list(map(float,input().split())))\nx_input=feature_scale(x_input)\n\npredict= np.dot(w,x_input)+b\nprint(\"the Predicted Price is:\", predict*100000)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T17:02:13.829809Z","iopub.execute_input":"2023-07-12T17:02:13.830260Z","iopub.status.idle":"2023-07-12T17:03:02.614481Z","shell.execute_reply.started":"2023-07-12T17:02:13.830228Z","shell.execute_reply":"2023-07-12T17:03:02.613020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression with two features - Regularised\n\n'''In this logistic regression vetorization is used as multiple features are included. In case of single feature normal * can be used.\n   This is a regularised model as lambda l is used. in case of non regularised model l can be neglected\n   \n   the model: sigmoid(z)=1/(1+np.exp(-z))===>f(x)\n   z=np.dot(w,x)+b\n   \n   cost_function(regularised)= (-1/m) summation of i=1 to m (y*log(f(x)) + (1-y)*log(1-f(x))) + (l/m) summation of j=1 to n (wj)**2\n   \n   Grdient descent:\n   \n   untill converges{\n   \n   w=w-(alpha* d_dw)    d_dw===> (1/m)* (summation of i=1 to m (f(x)-y)*x) + (l/m)*w\n   b=b-(alpha* d_db)    d_db===> (1/m)*  summation of i=1 to m (f(x)-y)\n   \n   }\n   \n   '''\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef feature_scale(x):\n    x_max=np.max(x, axis=0)\n    return x/x_max\n\ndef sigmoid(x,w,b):\n    z=np.dot(w.T, x.T)+b\n    return 1/(1+np.exp(-z))\n\n\ndef cost_function(x,y,w,b):\n    total_cost=0\n    m=len(x)\n    e=1e-7\n    for i in range(m):\n        f_wb=sigmoid(x[i],w,b)\n        total_cost+=(y[i]*np.log(f_wb+e)+(1-y[i])*np.log(1-f_wb+e))\n    total_cost/=-m\n    \n    return total_cost\n\ndef reg_cost(x,y,w,b,l):\n    m=len(x)\n    cost=cost_function(x,y,w,b)\n    reg=sum(np.square(w))\n    reg_cost=cost+((l/(2*m))*reg)\n    return reg_cost\n\ndef compute_gradient(x,y,w,b,l):\n    m,n=x.shape\n    \n    d_dw=np.zeros((n,))\n    d_db=0\n    \n    for i in range(m):\n        f_wb=sigmoid(x[i],w,b)\n        err=f_wb-y[i]\n        for j in range(n):\n            d_dw+=err*x[i,j] + (l*w[j])   \n        d_db+=err\n    d_dw/=m\n    d_db/=m\n    \n    return d_dw, d_db\n\ndef gradient_descent(x,y,w,b,alpha, iters,l):\n    j=[]\n    it=[]\n    prev_cost=reg_cost(x,y,w,b,l)\n    for i in range(iters):\n        d_dw, d_db= compute_gradient(x,y,w,b,l)\n        \n        w=w-(alpha*d_dw)\n        b=b-(alpha*d_db)\n        \n        curr_cost=reg_cost(x,y,w,b,l)\n        j.append(curr_cost)\n        it.append(i)\n        \n        if np.all(np.abs(prev_cost-curr_cost)<1e-7):\n            break\n            \n        prev_cost=curr_cost\n        \n    return w,b,j,it\n\ndef predict(x,w,b):\n    g=sigmoid(x,w,b)\n    \n    if g>=0.5:\n        return 1\n    else:\n        return 0\n\ndata = np.array([[34.62365962451697, 78.0246928153624, 0],\n                [30.28671076822607, 43.89499752400101, 0],\n                [35.84740876993872, 72.90219802708364, 0],\n                [60.18259938620976, 86.30855209546826, 1],\n                [79.0327360507101, 75.3443764369103, 1],\n                [45.08327747668339, 56.3163717815305, 0],\n                [61.10666453684766, 96.51142588489624, 1],\n                [75.02474556738889, 46.55401354116538, 1],\n                [76.09878670226257, 87.42056971926803, 1],\n                [84.43281996120035, 43.53339331072109, 1],\n                [95.86155507093572, 38.22527805795094, 0],\n                [75.01365838958247, 30.60326323428011, 0],\n                [82.30705337399482, 76.48196330235604, 1],\n                [69.36458875970939, 97.71869196188608, 1],\n                [39.53833914367223, 76.03681085115882, 0],\n                [53.9710521485623, 89.20735013750205, 1],\n                [69.07014406283025, 52.74046973016765, 1],\n                [67.94685547711617, 46.67857410673128, 0],\n                [70.66150955499435, 92.92713789364831, 1],\n                [76.97878372747498, 47.57596364975532, 1],\n                [67.37202754570876, 42.83843832029179, 0],\n                [89.67677575072079, 65.79936592745237, 1],\n                [50.534788289883, 48.85581152764205, 0],\n                [34.21206097786789, 44.20952859866288, 0],\n                [77.9240914545704, 68.9723599933059, 1],\n                [62.27101367004632, 69.95445795447587, 1],\n                [80.1901807509566, 44.82162893218353, 1],\n                [93.114388797442, 38.80067033713209, 0],\n                [61.83020602312595, 50.25610789244621, 0],\n                [38.78580379679423, 64.99568095539578, 0],\n                [61.379289447425, 72.80788731317097, 1],\n                [85.40451939411645, 57.05198397627122, 1],\n                [52.10797973193984, 63.12762376881715, 0],\n                [52.04540476831827, 69.43286012045222, 1],\n                [40.23689373545111, 71.16774802184875, 0],\n                [54.63510555424817, 52.21388588061123, 0],\n                [33.91550010906887, 98.86943574220611, 0],\n                [64.17698887494485, 80.90806058670817, 1],\n                [74.78925295941542, 41.57341522824434, 0],\n                [34.1836400264419, 75.2377203360134, 0],\n                [83.90239366249155, 56.30804621605327, 1],\n                [51.54772026906181, 46.85629026349976, 0],\n                [94.44336776917852, 65.56892160559052, 1],\n                [82.36875375713919, 40.61825515970618, 0],\n                [51.04775177128865, 45.82270145776001, 0],\n                [62.22267576120188, 52.06099194836679, 0],\n                [77.19303492601364, 70.45820000180959, 1],\n                [97.77159928000232, 86.7278223300282, 1],\n                [62.07306379667647, 96.76882412413983, 1],\n                [91.56497449807442, 88.69629254546599, 1],\n                [79.94481794066932, 74.16311935043758, 1],\n                [99.2725269292572, 60.99903099844988, 1],\n                [90.54671411399852, 43.39060180650027, 1],\n                [34.52451385320009, 60.39634245837173, 0],\n                [50.2864961189907, 49.80453881323059, 0],\n                [49.58667721632031, 59.80895099453265, 0],\n                [97.64563396007767, 68.86157272420604, 1],\n                [32.57720016809309, 95.59854761387875, 0],\n                [74.24869136721598, 69.82457122657193, 1],\n                [71.79646205863379, 78.45356224515052, 1],\n                [75.3956114656803, 85.75993667331619, 1],\n                [35.28611281526193, 47.02051394723416, 0],\n                [56.25381749711624, 39.26147251058019, 0],\n                [30.05882244669796, 49.59297386723685, 0],\n                [44.66826172480893, 66.45008614558913, 0],\n                [66.56089447242954, 41.09209807936973, 0],\n                [40.45755098375164, 97.53518548909936, 1],\n                [49.07256321908844, 51.88321182073966, 0],\n                [80.27957401466998, 92.11606081344084, 1],\n                [66.74671856944039, 60.99139402740988, 1],\n                [32.72283304060323, 43.30717306430063, 0],\n                [64.0393204150601, 78.03168802018232, 1],\n                [72.34649422579923, 96.22759296761404, 1],\n                [60.45788573918959, 73.09499809758037, 1],\n                [58.84095621726802, 75.85844831279042, 1],\n                [99.82785779692128, 72.36925193383885, 1],\n                [47.26426910848174, 88.47586499559782, 1],\n                [50.45815980285988, 75.80985952982456, 1],\n                [60.45555629271532, 42.50840943572217, 0],\n                [82.22666157785568, 42.71987853716458, 0],\n                [88.9138964166533, 69.80378889835472, 1],\n                [94.83450672430196, 45.69430680250754, 1],\n                [67.31925746917527, 66.58935317747915, 1],\n                [57.23870631569862, 59.51428198012956, 1],\n                [80.36675600171273, 90.96014789746954, 1],\n                [68.46852178591112, 85.59430710452014, 1],\n                [42.0754545384731, 78.84478600148043, 0],\n                [75.47770200533905, 90.42453899753964, 1],\n                [78.63542434898018, 96.64742716885644, 1],\n                [52.34800398794107, 60.76950525602592, 0],\n                [94.09433112516793, 77.15910509073893, 1],\n                [90.44855097096364, 87.50879176484702, 1],\n                [55.48216114069585, 35.57070347228866, 0],\n                [74.49269241843041, 84.84513684930135, 1],\n                [89.84580670720979, 45.35828361091658, 1],\n                [83.48916274498238, 48.38028579728175, 1],\n                [42.2617008099817, 87.10385094025457, 1],\n                [99.31500880510394, 68.77540947206617, 1],\n                [55.34001756003703, 64.9319380069486, 1],\n                [74.77589300092767, 89.52981289513276, 1]])\n\nx = data[:, :-1] \ny = data[:, -1]\n\nw=np.array([0.2,0.2])\nb=-24\n\nalpha= 0.0001\niters=10000\n\nl=1\n\nw,b,j,it=gradient_descent(x,y,w,b,alpha,iters,l)\n\nplt.plot(it,j)\nplt.show()\n\nx_input=np.array(list(map(float,input(\"Enter Two marks: \").split())))\n\nif predict(x_input,w,b)==1:\n    print(\"Admission Granted\")\nelse:\n    print(\"Admission Denied\")","metadata":{"execution":{"iopub.status.busy":"2023-07-19T08:04:48.121470Z","iopub.execute_input":"2023-07-19T08:04:48.121860Z","iopub.status.idle":"2023-07-19T08:05:49.729370Z","shell.execute_reply.started":"2023-07-19T08:04:48.121827Z","shell.execute_reply":"2023-07-19T08:05:49.728578Z"},"trusted":true},"execution_count":null,"outputs":[]}]}